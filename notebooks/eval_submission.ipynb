{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# Kaggle paths\n",
    "SOLVER_DIR = \"/kaggle/input/arc-ttt/arc_ttt\"\n",
    "EVAL_FILE = \"/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# Load API key from Kaggle secrets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = user_secrets.get_secret(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "sys.path.insert(0, SOLVER_DIR)\n",
    "os.chdir(SOLVER_DIR)\n",
    "\n",
    "from orchestrator import solve_with_judge\n",
    "from config import reset_usage_stats, get_usage_stats\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ARC TTT SOLVER - Evaluation Mode\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Ensemble Dir: {SOLVER_DIR}\")\n",
    "print(f\"Eval File: {EVAL_FILE}\")\n",
    "print(f\"Output Dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_CONCURRENT = 60\n",
    "\n",
    "SUBMISSION_FILE = os.path.join(OUTPUT_DIR, \"submission.json\")\n",
    "\n",
    "print(f\"Submission file: {SUBMISSION_FILE}\")\n",
    "\n",
    "submission_data = {}\n",
    "FALLBACK = [[0, 0], [0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tasks from single JSON file\n",
    "with open(EVAL_FILE) as f:\n",
    "    all_tasks = json.load(f)\n",
    "\n",
    "TASKS = list(all_tasks.keys())\n",
    "print(f\"Loaded {len(TASKS)} tasks from evaluation challenges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_submission_file(task_id: str, result: dict, task_data: dict):\n",
    "    \"\"\"Update submission file with task results.\"\"\"\n",
    "    global submission_data\n",
    "    \n",
    "    n_tests = len(task_data.get('test', []))\n",
    "    attempt_1 = result.get(\"attempt_1\")\n",
    "    attempt_2 = result.get(\"attempt_2\")\n",
    "    \n",
    "    preds = []\n",
    "    for i in range(n_tests):\n",
    "        if n_tests == 1:\n",
    "            a1 = attempt_1 if attempt_1 is not None else FALLBACK\n",
    "            a2 = attempt_2 if attempt_2 is not None else FALLBACK\n",
    "        else:\n",
    "            # Multi-test case handling\n",
    "            if isinstance(attempt_1, list) and len(attempt_1) > i and isinstance(attempt_1[i], list):\n",
    "                a1 = attempt_1[i]\n",
    "            else:\n",
    "                a1 = FALLBACK\n",
    "            if isinstance(attempt_2, list) and len(attempt_2) > i and isinstance(attempt_2[i], list):\n",
    "                a2 = attempt_2[i]\n",
    "            else:\n",
    "                a2 = FALLBACK\n",
    "        preds.append({\"attempt_1\": a1, \"attempt_2\": a2})\n",
    "    \n",
    "    submission_data[task_id] = preds\n",
    "    \n",
    "    # Save incrementally\n",
    "    with open(SUBMISSION_FILE, 'w') as f:\n",
    "        json.dump(submission_data, f)\n",
    "    \n",
    "    return len(submission_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "reset_usage_stats()\n",
    "completed = 0\n",
    "source_counts = {\"attempt_1\": {}, \"attempt_2\": {}}\n",
    "\n",
    "def process_task(task_id: str) -> dict:\n",
    "    \"\"\"Process a single task.\"\"\"\n",
    "    task_data = all_tasks[task_id]\n",
    "    # No ground truth for evaluation tasks\n",
    "    return solve_with_judge(task_data, task_id, ground_truths=None, verbose=False)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_CONCURRENT) as executor:\n",
    "    futures = {executor.submit(process_task, tid): tid for tid in TASKS}\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        task_id = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            result[\"task_id\"] = task_id\n",
    "            \n",
    "            # Track sources\n",
    "            a1_src = result.get(\"attempt_1_source\", \"none\")\n",
    "            a2_src = result.get(\"attempt_2_source\", \"none\")\n",
    "            source_counts[\"attempt_1\"][a1_src] = source_counts[\"attempt_1\"].get(a1_src, 0) + 1\n",
    "            source_counts[\"attempt_2\"][a2_src] = source_counts[\"attempt_2\"].get(a2_src, 0) + 1\n",
    "            \n",
    "            completed += 1\n",
    "            \n",
    "            # Update submission\n",
    "            task_data = all_tasks[task_id]\n",
    "            n_submitted = update_submission_file(task_id, result, task_data)\n",
    "            \n",
    "            print(f\"[{completed}/{len(TASKS)}] {task_id}: âœ“ | a1={a1_src} a2={a2_src} | {n_submitted} tasks saved\")\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if 'all_outputs' in result:\n",
    "                del result['all_outputs']\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(f\"[{completed}/{len(TASKS)}] {task_id}: ERROR - {e}\")\n",
    "            traceback.print_exc()\n",
    "            completed += 1\n",
    "            \n",
    "            # Still add fallback to submission\n",
    "            task_data = all_tasks[task_id]\n",
    "            n_tests = len(task_data.get('test', []))\n",
    "            submission_data[task_id] = [{\"attempt_1\": FALLBACK, \"attempt_2\": FALLBACK} for _ in range(n_tests)]\n",
    "            with open(SUBMISSION_FILE, 'w') as f:\n",
    "                json.dump(submission_data, f)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Tasks processed: {completed}/{len(TASKS)}\")\n",
    "print(f\"  Submission file: {SUBMISSION_FILE}\")\n",
    "print(f\"\\n  Attempt 1 Sources:\")\n",
    "for src, cnt in sorted(source_counts[\"attempt_1\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"    {src}: {cnt}\")\n",
    "print(f\"\\n  Attempt 2 Sources:\")\n",
    "for src, cnt in sorted(source_counts[\"attempt_2\"].items(), key=lambda x: -x[1]):\n",
    "    print(f\"    {src}: {cnt}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(get_usage_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission format\n",
    "with open(SUBMISSION_FILE) as f:\n",
    "    sub = json.load(f)\n",
    "\n",
    "print(f\"Submission contains {len(sub)} tasks\")\n",
    "print(f\"\\nSample entry:\")\n",
    "sample_id = list(sub.keys())[0]\n",
    "print(f\"  {sample_id}: {sub[sample_id]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
