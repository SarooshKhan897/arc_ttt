{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ARC TTT (Test-Time Training)\n",
        "\n",
        "Ensemble solver with 3 solvers x 2 runs each (6 candidates total):\n",
        "- **Perceiver**: Perception-based solver\n",
        "- **Phased**: Structured tool-calling solver  \n",
        "- **Iterative**: Code execution with iterative refinement\n",
        "\n",
        "Voting + Judge selects top 2 submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
        "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import glob\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "KAGGLE = Path(\"/kaggle/input\").exists()\n",
        "\n",
        "if KAGGLE:\n",
        "    ENSEMBLE_DIR = \"/kaggle/input/arc-ttt/arc_ttt\"\n",
        "    EVAL_DIR = \"/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges\"\n",
        "    OUTPUT_DIR = \"/kaggle/working\"\n",
        "    \n",
        "    try:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        user_secrets = UserSecretsClient()\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = user_secrets.get_secret(\"OPENROUTER_API_KEY\")\n",
        "        print(\"âœ“ API key loaded from Kaggle secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Failed to load API key from secrets: {e}\")\n",
        "        os.environ[\"OPENROUTER_API_KEY\"] = \"\"\n",
        "else:\n",
        "    ENSEMBLE_DIR = \"/path/to/arc_ttt\"\n",
        "    EVAL_DIR = \"/path/to/arc/evaluation\"\n",
        "    OUTPUT_DIR = ENSEMBLE_DIR\n",
        "    \n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = os.environ.get(\"OPENROUTER_API_KEY\", \"YOUR_API_KEY_HERE\")\n",
        "\n",
        "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
        "    raise ValueError(\"OPENROUTER_API_KEY not found. Set it as an environment variable or use Kaggle secrets.\")\n",
        "\n",
        "sys.path.insert(0, ENSEMBLE_DIR)\n",
        "os.chdir(ENSEMBLE_DIR)\n",
        "\n",
        "from orchestrator import solve_with_judge, solve_batch_with_judge\n",
        "from config import reset_usage_stats, get_usage_stats\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸš€ ARC TTT SOLVER\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Environment: {'Kaggle' if KAGGLE else 'Local'}\")\n",
        "print(f\"Ensemble Dir: {ENSEMBLE_DIR}\")\n",
        "print(f\"Eval Dir: {EVAL_DIR}\")\n",
        "print(f\"Output Dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_CONCURRENT = 60\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_FILE = os.path.join(OUTPUT_DIR, f\"results_{timestamp}.jsonl\")\n",
        "SUBMISSION_FILE = os.path.join(OUTPUT_DIR, f\"submission_{timestamp}.json\")\n",
        "\n",
        "print(f\"Results file: {RESULTS_FILE}\")\n",
        "print(f\"Submission file: {SUBMISSION_FILE}\")\n",
        "\n",
        "submission_data = {}\n",
        "FALLBACK = [[0, 0], [0, 0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_task(task_id: str) -> dict:\n",
        "    task_path = os.path.join(EVAL_DIR, f\"{task_id}.json\")\n",
        "    with open(task_path) as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def get_ground_truth_from_task(task_data: dict) -> list[np.ndarray]:\n",
        "    outputs = []\n",
        "    for test in task_data.get('test', []):\n",
        "        if 'output' in test:\n",
        "            outputs.append(np.array(test['output']))\n",
        "    return outputs if outputs else None\n",
        "\n",
        "all_task_files = glob.glob(os.path.join(EVAL_DIR, \"*.json\"))\n",
        "all_task_ids = [os.path.basename(f).replace(\".json\", \"\") for f in all_task_files]\n",
        "\n",
        "print(f\"Found {len(all_task_ids)} tasks in evaluation directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TASKS = all_task_ids\n",
        "\n",
        "print(f\"Running on {len(TASKS)} tasks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_attempt(attempt, ground_truths: list[np.ndarray]) -> tuple[bool, int, int]:\n",
        "    if attempt is None or ground_truths is None or len(ground_truths) == 0:\n",
        "        return (False, 0, len(ground_truths) if ground_truths else 0)\n",
        "    \n",
        "    n_tests = len(ground_truths)\n",
        "    \n",
        "    try:\n",
        "        if n_tests == 1:\n",
        "            attempt_arr = np.array(attempt)\n",
        "            gt = ground_truths[0]\n",
        "            correct = attempt_arr.shape == gt.shape and np.array_equal(attempt_arr, gt)\n",
        "            return (correct, 1 if correct else 0, 1)\n",
        "        else:\n",
        "            is_list_of_grids = (\n",
        "                isinstance(attempt, list) and \n",
        "                len(attempt) > 0 and \n",
        "                isinstance(attempt[0], list) and \n",
        "                len(attempt[0]) > 0 and\n",
        "                isinstance(attempt[0][0], list)\n",
        "            )\n",
        "            \n",
        "            if not is_list_of_grids:\n",
        "                attempt_arr = np.array(attempt)\n",
        "                gt = ground_truths[0]\n",
        "                correct = attempt_arr.shape == gt.shape and np.array_equal(attempt_arr, gt)\n",
        "                return (False, 1 if correct else 0, n_tests)\n",
        "            else:\n",
        "                tests_correct = 0\n",
        "                for i, gt in enumerate(ground_truths):\n",
        "                    if i < len(attempt):\n",
        "                        attempt_arr = np.array(attempt[i])\n",
        "                        gt_arr = np.array(gt)\n",
        "                        if attempt_arr.shape == gt_arr.shape and np.array_equal(attempt_arr, gt_arr):\n",
        "                            tests_correct += 1\n",
        "                return (tests_correct == n_tests, tests_correct, n_tests)\n",
        "    except Exception:\n",
        "        return (False, 0, n_tests)\n",
        "\n",
        "\n",
        "def calculate_task_score(result: dict, ground_truths: list[np.ndarray]) -> tuple[float, float, float]:\n",
        "    if ground_truths is None or len(ground_truths) == 0:\n",
        "        return (0.0, 0.0, 0.0)\n",
        "    \n",
        "    attempt_1 = result.get(\"attempt_1\")\n",
        "    attempt_2 = result.get(\"attempt_2\")\n",
        "    \n",
        "    _, tests_correct_1, total_tests = check_attempt(attempt_1, ground_truths)\n",
        "    _, tests_correct_2, _ = check_attempt(attempt_2, ground_truths)\n",
        "    \n",
        "    score_1 = tests_correct_1 / total_tests if total_tests > 0 else 0.0\n",
        "    score_2 = tests_correct_2 / total_tests if total_tests > 0 else 0.0\n",
        "    final_score = max(score_1, score_2)\n",
        "    \n",
        "    return (score_1, score_2, final_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "reset_usage_stats()\n",
        "results = []\n",
        "total_score = 0\n",
        "score_1_total = 0\n",
        "score_2_total = 0\n",
        "partial_count = 0  \n",
        "completed = 0\n",
        "\n",
        "source_counts = {\"attempt_1\": {}, \"attempt_2\": {}}\n",
        "solver_wins = {\"perceiver_1\": 0, \"perceiver_2\": 0, \"phased_1\": 0, \"phased_2\": 0, \"iterative_1\": 0, \"iterative_2\": 0}\n",
        "\n",
        "def update_submission_file(task_id, result, task_data):\n",
        "    global submission_data\n",
        "    \n",
        "    n_tests = len(task_data.get('test', []))\n",
        "    attempt_1 = result.get(\"attempt_1\")\n",
        "    attempt_2 = result.get(\"attempt_2\")\n",
        "    \n",
        "    preds = []\n",
        "    for i in range(n_tests):\n",
        "        if n_tests == 1:\n",
        "            a1 = attempt_1 if attempt_1 is not None else FALLBACK\n",
        "            a2 = attempt_2 if attempt_2 is not None else FALLBACK\n",
        "        else:\n",
        "            if isinstance(attempt_1, list) and len(attempt_1) > i and isinstance(attempt_1[i], list):\n",
        "                a1 = attempt_1[i]\n",
        "            else:\n",
        "                a1 = FALLBACK\n",
        "            if isinstance(attempt_2, list) and len(attempt_2) > i and isinstance(attempt_2[i], list):\n",
        "                a2 = attempt_2[i]\n",
        "            else:\n",
        "                a2 = FALLBACK\n",
        "        preds.append({\"attempt_1\": a1, \"attempt_2\": a2})\n",
        "    \n",
        "    submission_data[task_id] = preds\n",
        "    \n",
        "    with open(SUBMISSION_FILE, 'w') as f:\n",
        "        json.dump(submission_data, f)\n",
        "    \n",
        "    return len(submission_data)\n",
        "\n",
        "task_cache = {}\n",
        "gt_cache = {}\n",
        "\n",
        "def process_task(task_id):\n",
        "    task_data = load_task(task_id)\n",
        "    gt = get_ground_truth_from_task(task_data)\n",
        "    task_cache[task_id] = task_data\n",
        "    gt_cache[task_id] = gt\n",
        "    return solve_with_judge(task_data, task_id, gt, verbose=False)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_CONCURRENT) as executor:\n",
        "    futures = {executor.submit(process_task, tid): tid for tid in TASKS}\n",
        "    \n",
        "    for future in as_completed(futures):\n",
        "        task_id = futures[future]\n",
        "        try:\n",
        "            result = future.result()\n",
        "            result[\"task_id\"] = task_id\n",
        "            results.append(result)\n",
        "            \n",
        "            # Calculate score using cached ground truth\n",
        "            gt = gt_cache.get(task_id) or get_ground_truth_from_task(load_task(task_id))\n",
        "            score_1, score_2, task_score = calculate_task_score(result, gt)\n",
        "            total_score += task_score\n",
        "            score_1_total += score_1\n",
        "            score_2_total += score_2\n",
        "            \n",
        "            # Track sources\n",
        "            a1_src = result.get(\"attempt_1_source\", \"none\")\n",
        "            a2_src = result.get(\"attempt_2_source\", \"none\")\n",
        "            source_counts[\"attempt_1\"][a1_src] = source_counts[\"attempt_1\"].get(a1_src, 0) + 1\n",
        "            source_counts[\"attempt_2\"][a2_src] = source_counts[\"attempt_2\"].get(a2_src, 0) + 1\n",
        "            \n",
        "            # Track which solver won\n",
        "            if score_1 == 1.0:\n",
        "                solver_wins[a1_src] = solver_wins.get(a1_src, 0) + 1\n",
        "            elif score_2 == 1.0:\n",
        "                solver_wins[a2_src] = solver_wins.get(a2_src, 0) + 1\n",
        "            \n",
        "            completed += 1\n",
        "            \n",
        "            scores_detail = result.get(\"scores\", {})\n",
        "            a1_scores = scores_detail.get(\"attempt_1\", {})\n",
        "            a2_scores = scores_detail.get(\"attempt_2\", {})\n",
        "            a1_partial = f\" ({a1_scores.get('tests_correct', 0)}/{a1_scores.get('total_tests', 1)})\" if a1_scores.get(\"partial\") else \"\"\n",
        "            a2_partial = f\" ({a2_scores.get('tests_correct', 0)}/{a2_scores.get('total_tests', 1)})\" if a2_scores.get(\"partial\") else \"\"\n",
        "            \n",
        "            if 0 < task_score < 1.0:\n",
        "                partial_count += 1\n",
        "            \n",
        "            if task_score == 1.0:\n",
        "                status = \"âœ“ PASS\"\n",
        "            elif task_score > 0:\n",
        "                status = \"âš  PARTIAL\"\n",
        "            else:\n",
        "                status = \"âœ— FAIL\"\n",
        "            print(f\"[{completed}/{len(TASKS)}] {task_id}: {status} (s1={score_1:.2f} s2={score_2:.2f} best={task_score:.2f}) | a1={a1_src} a2={a2_src}\")\n",
        "            \n",
        "            with open(RESULTS_FILE, \"a\") as f:\n",
        "                f.write(json.dumps({\n",
        "                    \"task_id\": task_id,\n",
        "                    \"attempt_1\": result.get(\"attempt_1\"),\n",
        "                    \"attempt_2\": result.get(\"attempt_2\"),\n",
        "                    \"attempt_1_source\": a1_src,\n",
        "                    \"attempt_2_source\": a2_src,\n",
        "                    \"ratings\": [(r[0], r[1], r[2][:100] if len(r) > 2 and r[2] else \"\") for r in result.get(\"ratings\", [])],\n",
        "                    \"score_1\": score_1,\n",
        "                    \"score_2\": score_2,\n",
        "                    \"final_score\": task_score,\n",
        "                    \"scores\": result.get(\"scores\", {}),  # Detailed scores with tests_correct, total_tests, partial\n",
        "                    \"num_distinct\": result.get(\"num_distinct\", 0),\n",
        "                    \"total_candidates\": result.get(\"total_candidates\", 6),\n",
        "                    \"distinct_outputs\": result.get(\"distinct_outputs\", []),\n",
        "                    \"judge_used\": result.get(\"judge_used\", False),\n",
        "                    \"timing\": result.get(\"timing\"),\n",
        "                    \"usage\": result.get(\"usage\"),\n",
        "                }) + \"\\n\")\n",
        "            \n",
        "            # Update submission file (auto-updates as each task completes)\n",
        "            task_data = task_cache.get(task_id) or load_task(task_id)\n",
        "            n_submitted = update_submission_file(task_id, result, task_data)\n",
        "            print(f\"    ðŸ’¾ Submission updated: {n_submitted} tasks\")\n",
        "            \n",
        "            # Flush memory after each task\n",
        "            if 'all_outputs' in result:\n",
        "                del result['all_outputs']  # Large object no longer needed\n",
        "            if task_id in task_cache:\n",
        "                del task_cache[task_id]  # Clear cached task data\n",
        "            gc.collect()\n",
        "            \n",
        "        except Exception as e:\n",
        "            import traceback\n",
        "            print(f\"[{completed}/{len(TASKS)}] {task_id}: ERROR - {e}\")\n",
        "            traceback.print_exc()\n",
        "            completed += 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"  Score 1 (Attempt 1): {score_1_total:.2f}/{len(TASKS)} = {score_1_total/len(TASKS)*100:.1f}%\")\n",
        "print(f\"  Score 2 (Attempt 2): {score_2_total:.2f}/{len(TASKS)} = {score_2_total/len(TASKS)*100:.1f}%\")\n",
        "print(f\"  Combined Score:      {total_score:.2f}/{len(TASKS)} = {total_score/len(TASKS)*100:.1f}%\")\n",
        "print(f\"  Partial Tasks:       {partial_count}/{len(TASKS)} (tasks with 0 < score < 1)\")\n",
        "print(f\"\\n  Solver Win Counts:\")\n",
        "for solver, wins in sorted(solver_wins.items(), key=lambda x: -x[1]):\n",
        "    print(f\"    {solver}: {wins}\")\n",
        "print(f\"\\n  Attempt 1 Sources:\")\n",
        "for src, cnt in sorted(source_counts[\"attempt_1\"].items(), key=lambda x: -x[1]):\n",
        "    print(f\"    {src}: {cnt}\")\n",
        "print(f\"\\n  Attempt 2 Sources:\")\n",
        "for src, cnt in sorted(source_counts[\"attempt_2\"].items(), key=lambda x: -x[1]):\n",
        "    print(f\"    {src}: {cnt}\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(get_usage_stats())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
